<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Institutional Ethics</title>
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
	<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
	<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
	<link rel="stylesheet" type="text/css" href="login.css">
    <link rel="stylesheet" type="text/css" href="./resources/css/wrtg.css">
	<script type="text/javascript" src="./login.js"></script>
</head>

<body>
    <nav class="navbar-dark bg-dark d-flex justify-content-left" >
        <a class="nav-link" href="InstitutionalEthics.html">Home</a>
      </nav>
      <div style="margin-left:auto;margin-right:auto;">
        
      </div>
      
	<div class="jumbotron" style="width:66%; height:auto; margin-left:auto; margin-right:auto;">
		<div class="container justify-content-right" style="max-width: 75%; margin-top:-15px;">
      <img src="resources/img/ai-banner.png" class="img-fluid" alt="Responsive image" style="margin-left:auto;margin-right:auto;">
      <h1>Ethical Concerns Regarding Data Collection, Distribution, and Its Applications</h1>
      <h6 style="padding-top:10px;color:gray;">April 21st, 2021</h6>
      <h6 style="padding-top:10px;color:gray;">WRTG3035 Institutional Ethics -- Andrew Settergren</h6>
      

      <p style="font-size:20px; padding-top:20px;">Over the last decade, we have been introduced to a new era of technology and information with the rise in Big Data and Artificial Intelligence. 
        As we are gaining more familiarity with ways we can store, collect, and distribute data – unique artificial intelligence applications are being built with this data. 
        While some of these applications are revolutionizing science, technology, products, and services – others raise major economical, ethical, and moral concerns. In this article, 
        I would like to introduce some of the methods and applications in industry that undermine Stuart Mill’s Theory of Utilitarianism and Kantian Theory while providing potential solutions 
        to preventing or mitigating said cases. </p>

      <p style="font-size:20px; padding-top:20px;">Growth tactics used in social media services defy Kantian Theory as these services treat their users as a means to an end, that is to increase monetization and usability while exploiting 
        natural human behavior.  In the documentary “The Social Dilemma” (Sandy Parakilas, 2020), Sandy describes how growth tactics are being developed and used in large corporations like Facebook 
        by collecting and analyzing data by “using scientific A/B testing on small feature changes”, and measuring the impact it had on its users. By rolling out thousands of these experiments on users 
        and utilizing artificial intelligence to find correlations in this data they are collecting, companies like Facebook are able to figure out what makes the user engage more with their application 
        and in turn, generate more revenue for the company. This data and application of artificial intelligence isn’t being used as a means to better off it’s people, but rather used as a method to 
        manipulate users into using the application more, respond more to advertisements, and ultimately to get more users hooked on their social media service so that Facebook can generate more revenue 
        and collect more user data. As Sandy had described, “we’re all lab rats … it’s not like they’re trying to benefit us, they just want us to stare at ads more frequently so that they can make more money”.  
        Had Facebook been transparent about this kind of data collection and user testing, or some board existed within Facebook that looked into oversight of the company’s products, services, and development 
        methodologies, it is likely that these growth tactics used in social media services would have to be reconsidered as a means to benefit its people and users – not to use its users as a means to 
        their end. </p>

        <p style="font-size:20px; padding-top:20px;">While social media services work to get you addicted to their applications, some other companies such as 23andMe’s use electronic health records (EHRs) to collect and sell genetic information to pharmaceutical 
          companies in a manner that again undermines Kantian Theory. This company at the time was not subject to the HIPAA act that prohibits “using or disclosing protected health information that is genetic information
           for underwriting purposes in all plans covered in the HIPAA privacy rule” (Hipaa Omnibus Rule 2013). As a result, 23andMe sold 300 million dollars’ worth of genetic data to GlaxoSmithKline plc, a company 
           located in Brentford, England that manufactures their own drugs. Lots of controversy arose over this business transaction as many users feel like there was a lack of transparency and didn’t fully acknowledge 
           that their data was being sold to a pharmaceutical company for research and development of drugs. Not only did the company just blatantly profit from customers valuable and private electronic health records, 
           but their customers didn’t get any compensation and are more importantly at risk of their data being sold to other companies that could misuse this information. Thus, it is particularly important to ensure 
           there is oversight, transparency, and consent in companies that have access or are obtaining EHRs – especially when they are selling or distributing this data. </p>

        <p style="font-size:20px; padding-top:20px;">Contrary to the previous sections arguing the negative use cases of data collection, distribution, and its applications, Utilitarianism can be used to argue how artificial 
          intelligence can increase the amount of good in the world while decreasing the amount of bad with enough oversight in place. In the article “As governments adopt artificial 
          intelligence, there’s little oversight and lots of danger” (Hendler, 2019), Hendler talks about how public knowledge on artificial intelligence lead to the halt in utilizing insights 
          gained from artificial intelligence in cases such as the Flint Water Crisis in Flint, Michigan. A team of engineers and government employees worked to analyze data and create an algorithm 
          that could predict the number of pipes needed to be dug up and replaced in Flint Michigan. Despite the promising results from the system they had made, city officials ended up rejecting the 
          use of this artificial intelligence model/system as they didn’t completely understand how the model made the assumptions it had made. This was due to a lack of oversight within this project, 
          and had oversight been in place to verify the results and accuracy of the model, the project overall could have sped up the time it took to replace lead piping and the cost of the operation. 
          In cases like this, it is important to recognize that with enough oversight in data collection, distribution, and applications – these practices could actually do more good for society than it 
          does bad.</p>

        <p style="font-size:20px; padding-top:20px;">Ultimately there are always going to be examples of ethically sound/unsound data collection, distribution, and applications. With Kantian Theory, Utilitarianism, and the adoption of these moral 
          theories into oversight engineering - these practices could continue to bring exciting breakthroughs in medicine, environmental sciences, and technology. It is apparent that these ethical concerns 
          that are now surfacing as a result of these practices, and that there are ethical practices that need to be set in place when any company, government, or institution is collecting, selling, or creating 
          applications out of their users data.  </p>
		</div>
	</div>
	<div class="jumbotron" style="width:66%; height:auto; margin-left:auto; margin-right:auto;">
		<div class="container justify-content-right" style="max-width: 75%; margin-top:-15px;">
      <h6 style="padding-top:10px;color:gray;">References:</h6>
      <p>Carpenter, Aren. “The Ethics of Data Collection.” Medium, Towards Data Science, 10 Aug. 2020, towardsdatascience.com/the-ethics-of-data-collection-9573dc0ae240. </p>
      <p>Porsdam Mann, Sebastian, et al. “Facilitating the Ethical Use of Health Data for the Benefit of Society: Electronic Health Records, Consent and the Duty of Easy Rescue.” Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences, The Royal Society, 28 Dec. 2016, www.ncbi.nlm.nih.gov/pmc/articles/PMC5124071/. </p>
      <p>The Social Dilemma, Jeff Orlowski, Netflix, September 2020, interviews with Tristan Harris, Sandy Parakilas</p>
    </div>
	</div>





	<!-- Ignore the code below. All of this is just for the code description -->
	<!-- End of -->
</body>

</html>
